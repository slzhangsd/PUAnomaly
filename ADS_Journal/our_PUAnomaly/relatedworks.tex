\section{Related Work}
\label{sec:related}
In this section, we will introduce recent work related to \name{} in three aspects. The first aspect is to introduce the research related to KPI anomaly detection algorithms (Section~\ref{subsec:KPI anomaly detection algorithms}). The second aspect is to introduce PU learning methods(Section~\ref{subsec:pu}). And the final aspect is an introduction to work related to active learning methods (Section~\ref{subsec:active}).

\subsection{KPI Anomaly Detection Algorithms}
\label{subsec:KPI anomaly detection algorithms}
Anomaly detection on the KPI streams refers to identifying unexpected data points from normal behavior. Over the years, lots of studies on anomaly detection for time series have been proposed. The algorithms can be divided into supervised learning methods, unsupervised learning methods, and semi-supervised learning methods.

\subsubsection{Supervised Learning Methods}
Supervised learning methods need operators' manual labels of KPI anomalies to learn algorithm selection and parameter tuning. To name some representative, EGADS~\cite{egads} separates forecasting, anomaly detection, and alerting into three separate components and uses AdaBoost~\cite{freund1997decision} to select the most relevant anomalous data points. Opprentice~\cite{liu2015opprentice} ensembles 14 widely-used traditional statistical algorithms with 133 enumerated configurations of hyper-parameters for these algorithms to extract features for the points. Then it trains a classifier using Random Forest. However, manually labeling anomalies for millions of KPI streams is not feasible.

\subsubsection{Unsupervised Learning Methods}
Unsupervised learning has emerged as a promising field in KPI anomaly detection, which does not require manual labeling. Isolation Forest~\cite{ding2013anomaly} assumes that the anomalous data points are few and different, then constructs tree structure to separate the points from the rest of points until all are isolated. The points closer to the root of the tree will be regarded as anomaly points.
Donut~\cite{xu2018unsupervised}, which is based on VAE, a deep bayesian model performs superior in accuracy. This method focuses on normal patterns instead of anomalies and tries to learn the probability distribution of the normal data points. Some other unsupervised based methods, such as Buzz~\cite{chen2019unsupervised} and Bagel~\cite{li2018robust} like Donut are also based on unsupervised learning methods to detect anomalies.

\subsubsection{Semi-supervised Learning Methods}
Semi-supervised learning~\cite{zhou2014semi} is halfway between supervised and unsupervised learning. These methods use unlabelled data to modify either parameters or models obtained from labeled data alone to maximize the learning performance of the models. ~\cite{Malhotra2015LongST} used stacked LSTM networks trained on non-anomalous data as a predictor to detect anomaly in time series. ~\cite{HighDimensional} used a combination of an one-class SVM model and a deep learning model to detect anomaly. ADS~\cite{ADSarticle} used clustering and CPLE~\cite{loog2016contrastive} to detect anomaly in time series. Although semi-supervised learning based methods do reduce the cost of manually labeling, it is very difficult for operators to label all the anomalous data points in those specified time series segments and ensure accuracy.

\subsection{PU Learning Methods}
\label{subsec:pu}
PU learning is suitable for lots of applications in text detection~\cite{Li2014SpottingFR, Liu2002PartiallySC, putranditional2003, ren-etal-2014-positive} and  bioscience~\cite{Mordelet_2011, Yang2014EnsemblePU}. ~\cite{putranditional2003}firstly regarded the unlabeled samples as negative samples and used a spy technique to introduce some positive instances to mixed sets and then got the threshold score between positive and negative classes.  ~\cite{Liu2002PartiallySC}proposed a technique which combines the Rocchio method and the SVM technique for classifier building, while the first step is also treating all unlabeled samples as negative set. 
It is not feasible to assume that all unlabeled samples are negative samples, because most of the training data in time series anomaly detection is positive. ~\cite{PUlearning2017} improved previous PU learning methods so that it could apply for time series anomaly detection and made it suitable for the unbalanced scenario. In our \name{}, we utilize the PU learning based~\cite{PUlearning2017} to label abnormalities as few as possible and achieve better results than the semi-supervised learning methods.

\subsection{Active Learning}
\label{subsec:active}
In \name{}, active learning is used to label unlabeled samples, thereby helping the PU learning algorithm to obtain more reliable normal samples. ~\cite{Outlieractive}used a selection strategy based on active learning to reduce the classification problem of anomaly detection. ~\cite{Pelleg04activelearning}proposed a novel active learning method to identify rare category records in an unlabelled noisy set with a small budget of data points that they are prepared to categorize. Most of the active learning methods such as~\cite{activelearning2015} selected samples on the classification boundary to label, while via experiments in our KPI streams, choosing the unlabeled data likely to be normal to label is the best selection strategy which is more suitable for KPI streams data.